---
title: Markov Chains
author: Dhruva Sambrani
date: 25 August, 2022
geometry: margin=1in
toc: true
---

\pagebreak

# Definition

**Equi 1**:
${X_n}_{n\ge 1}$ is a Markov Chain on a state space $S$ (countable) with an initial distribution $\lambda$ and transition matrix $P$ if

1. $P(x_0 = i) = \lambda_i$
2. **Markov property**: $P(x_{m+1} = i_{m+1} | \text{PAST}) = P(x_{m+1} = i_{m+1} | x_m=i_m) = p_{i_m i_{m+1}}$

**Equi 2**: Given $x_m$ the future $\{x_n: n > m\}$ and the past $\{x_n: n < m\}$ are independent.

**Equi 3**: $\{x_n\}$ is a $\text{MC}(\lambda, P)$ if $P(x_0=i_0, \dots x_m=i_m) = \lambda p_{i_0i_1} p_{i_1i_2} \dots$


## Proof of equivalence

### Showing 3 $\implies$ 1

**Equi 3** $\implies$ **Equi 1.1** is obvious.

$$P(x_m=i_m | \text{PAST}) = P(x_m=i_m, \text{PAST})/P(\text{PAST})$$

From **Equi 3**, 

$$P(x_m=i_m | \text{PAST}) = \frac{\lambda_{i_0} \prod_{k=1}^{m}p_{i_{k-1},i_{k}}}{\lambda \prod_{k=1}^{m-1} p_{i_{k-1},i_{k}}} = p_{i_{m-1}, i_m}$$

which is **Equi 1.2**. 

Hence **Equi 3** $\implies$ **Equi 1**

### Showing 1 $\implies$ 3

$$P(x_m=i_m, \text{PAST}) = P(x_m=i_m|\text{PAST}) P(\text{PAST})$$

From **Equi 1.2**:

$$P(x_m=i_m | \text{PAST}) = p_{i_{m-1}, i_{m}}$$
$$\implies P(x_m=i_m, \text{PAST}) = p_{i_{m-1}, i_{m}}P(\text{PAST})$$

Now similarly pulling out each step from the past into the product, we get 

$$P(x_0=i_0, \dots x_m=i_m) = P(x_0=i_0) \prod_{\substack{k=m\\ \Delta k = -1}}^{1}p_{i_{k-1},i_{k}}$$

Finally, using **Equi 1.1**, we get **Equi 3**.

# Transition Matrix

$$P = ((p_{ij}))_{i,j \in S}$$

where $p_{ij} =$ probability that the chain jumps to state $j$ if it is in state $i$.

## Stochasticity

Row-wise sum is 1. $\sum_j p_{ij}$ is the sum of the probability that given we are at $i$, we jump to any possible $j$. Since we must be _somewhere_ every step, this sum must be 1.


## Chapman Kolmogorov equation / Semigroup Property

$P^(n+m) = P^{n}P^{m} \forall n,m >= 0$

$$\begin{aligned}
p_{ij}^{(n+m)} &= P(X_{n+m} = j | X_0 = i)\\
            &= \sum_k P(X_{n+m}=j, X_m=k | X_0=i)\\
            &= \sum_k P(X_{n+m}=j | X_m=k , X_0=i) P(X_m=k | X_0=1)\\
            &= \sum_k P(X_{n+m}=j | X_m=k) P(X_m=k | X_0=1)\\
            &= \sum_k p^{m}_{ik} p_{kj}^{n}\\
\implies P^{n+m} &= P^nP^m
\end{aligned}$$

Going back to the example, 

$$P = (1-\alpha, \alpha; \beta, 1-\beta)$$

$$\begin{aligned}
    p_{11}^{(n)} &= \sum_j p_{1j}^{n-1} p_{j1}\\
                 &= p_{11}^{n-1}p_{11} + p_{12}^{n-1} p_{21}\\
                 &= p_{11}^{n-1} (1-\alpha) + \beta (1-p_{11}^{n-1})\\
\end{aligned}$$

> Exercise:
> 
> Similarly solve for other terms and find the values 

# Stationary distribution of an MC

**Defn**: A stationary distribution on the nodes of the MC is such that $(x_0, ... x_n)$ has the same distribution as $(x_m, ..., x_{m+n})$ for all $m$. That is, $X_m \sim X_l$ for any $m$ and $l$.

$\mu_0(i) = P(x_0 = i) \forall i \in S$

$\mu_n(i) = P(x_n = i)$

$mu_1(i) = mu_0(i) * p_ji$

Which is $mu_i = mu_0 P^i$

A distribution $\pi$ on S is called Stationary / invariant distribution of the chain $\text{MC}(P)$ is $\pi = \pi P$
That is, $\pi$ is a left eigenvector of $P$ with eigenvalue 1.

**Equi**:
S is finite, |S| = N, pi in |R^N_+ is called a stationary or invariant distn of the MS(P) if it satisfies
1. Balance Condition: piP = pi
2. pi mathbf 1 = 1

> Exercise:
> Ehrenfest chain
> 
> Chain of length N.\
> $P(X_{n+1} = i+1 | X_n=i) = (N-i)/N; P(X_{n+1} = i-1 | X_n = i) = i/N$\
> Find $\pi$.


### Example - 2 State MC

For $P = \begin{bmatrix}
1-\alpha & \alpha\\
\beta & 1-\beta
\end{bmatrix}$, what is the stationary distribution? What are the entries of $P^n$?

Solved in Assignment 1.

### Example - Bus Stop

Buses arrive at a bus stop st the inter-arrival times are iid. At time $n$, $x_n$ is the time until the next bus arrives

$p_{i+1,i} = 1,\ p_{1,i} = q(i)$

$\pi P = \pi$

$$P = \begin{bmatrix}
q(1)    &q(2)   &q(3) &\dots\\
 1      &0      &0    &\dots\\
 0      &1      &0    &\dots\\
 \dots
\end{bmatrix}$$

Th balance equation also leads to 

$$\pi(i) = \pi(i+1) + \pi(1)q(i)$$

Normalization leads to 

$$\sum_{i \in S} \pi(i) = 1 => \pi(1) sum_i sum_{j\ge i} q(j) = 1$$

$$pi(1) = \frac{1}{sum_i sum_{j\ge i} q(j)}$$

For stationary distribution to exist, we need the double sum to be finite, else $\pi(1) = 0$ which implies that $pi(i)=0$

$$sum_i sum_{j\ge i} q(j) = sum_j jq(j) = E[\text{time between arrival}]$$

## Flow of a MC

Defn: For $A \subset S$, define $F(A, A^C) = \sum_{i\in A} \sum_{j in A^C} \pi(i) p_{ij}$

Thm: $\pi$ satisfies the balance equation iff $F(A, A^C) = F(A^C, A) \forall A \subset S$

Proof: Suppose thm holds $forall A \subset S$. For $A=\{x\}$,

1. $F(A A^C) = \sum_{j \neq k} \pi(k) p_{kj}$
2. $F(A^C A) = \sum_{i \neq k} \pi(i) p_{ik}$

Then, 

$$\sum_{j\neq k} \pi(k) p_{kj} = sum_{i\neq k} \pi(i) p_{ik}$$

$$sum_{j\neq k} \pi(k) p_{kj} = sum_{i \in S} \pi(i) p_{ik} - \pi(k)p_{kk}$$

$$\implies \pi(k) = sum_{j in S} p_{kj} = sum_{i in S} \pi(i) p_{ik}$$
$$\implies \pi(k) = sum_{i in S} \pi(i) p_{ik}$$


Conversely, $\pi(i) = \sum_{j \in S} \pi(j) p_{ji}$
$\sum_{j\in A} \pi(i)p_{ij} + sum_{j in A^C} \pi(i)p_{ij} = \sum_A pi(j)p_{ji} + sum_{A^C} \pi(j) p_{ji}$

Now, sum over $i \in A$ on both sides and conclude

> Exercise: Consider the Gambler's Ruin MC with $M=\infty$, reflecting boundary condition at 0.
> Take $A = \{0, 1, .. n-1\}$ Write $F(A,A^C) = F(A^C,A)$ and solve for $\pi$

From the balance equation

$pi(i) = p pi(i+1) + q pi(i-1)$

From the flow equation, 

Let $A = \{0, 1, .. i-1\}$ 

$F(A, A^C) = p \pi(i-1) = q \pi(i) = F(A^C, A)$

This along with the normalization condition allows us to find $\pi$

## Class structure of a MC

Def: $i, j \in S, i \longrightarrow j$ if there exists $n \ge 0 st p_{ij}^{(n)} > 0  \implies \exists \text{PATH from } i \text{ to } j$. $\longrightarrow$ is transitive and reflexive.

Def: $\leftrightarrow: i \leftrightarrow j$, or "i communicates with j" iff $i \longrightarrow j$ and $j \longrightarrow i$. $\leftrightarrow$ is an equivalence relation.

Note, $S = \bigsqcup_i C_i$ where $C_i$s are called communicating classes.

### Closed communicating Classes

Def: $\sum_{j \in C} p_{ij} = 1 \forall i \in C$

If $C$ is a closed communicating class then, if $i \in C$ and $i \longrightarrow j \implies j \in C$

Thm: If C is a closed communicating class of MC($P$) then C is a closed communicating class of MC($P^n$)

Proof:

If $i \in C$ and $i \rightarrow j$ in MC($P^n$) $\exists \text{PATH}_{i \to j}$

### Irreducable

If a chain has only one closed communicating class, it is called irreducable. 

For any $i,j \in S \exists n>0 st p_{ij}^{(n)} > 0$. If $\{i\}$ is a closed communicating class $i$ is called an absorbing state.

## Period of an MC

Note that

$p_{ij}^{(nk)} \geq (p_{ii}^{(n)})^k$

from Chapman Kolmogorov theorem,

$p_{ii}^{(m)} > 0 => p_{ii}^{(n)} > 0 \text{if } m | n$

Period of $i$ is defined as $d(i) = gcd\{n : p_{ii}^{(n)} > 0\}$

$i$ is called aperiodic if $d(i) = 1$

### Period is a class property

If $i$ and $j$ are in same communicating class, $i \leftrightarrow j$, then $d(i) = d(j)$

$D_i = {n : p_{ii}^n > 0}; d_i = \gcd(D_i)$
$D_j = {n : p_{jj}^n > 0}; d_j = \gcd(D_j)$

Since $i \leftrightarrow j \implies \exists n_1, n_2, st p_{ij}^{(n_1)}, p_{ji}^{(n_2)} >= 0$ 

Note, $d_i$ and $d_j$ both divide $n_1+n_2$

For any $n$ in $D(i)$,

$p_{jj}^{(n_1+n_2+n)} \ge p_{ji}^{(n_2)} p_{ii}^{(n)} p_{ij}^{(n_1)} > 0$

$\implies d_j | n1+n2+n \implies d_j | n$

$\implies d_j | n \forall n \in D_i$
$\implies d_j \le d_i$

Similarly $d_i \le d_j$.

Hence $d_i = d_j$.

### Theorem

If $i \in S$ be aperiodic, then there exists $n_0$ st $p_{ii}^{(n)} > 0 \forall n \ge N$.

> Exercise: An irreducable chain is aperiodic iff $\exists n st p_{ij}^{(n)} > 0 \forall i,j \in S$

