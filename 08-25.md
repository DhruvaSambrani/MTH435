---
title: Markov Chains
author: Dhruva Sambrani
date: 25 August, 2022
geometry: margin=1in
toc: true
---

\pagebreak

# Definition

**Equi 1**:
${X_n}_{n\ge 1}$ is a Markov Chain on a state space $S$ (countable) with an initial distribution $\lambda$ and transition matrix $P$ if

1. $P(x_0 = i) = \lambda_i$
2. **Markov property**: $P(x_{m+1} = i_{m+1} | \text{PAST}) = P(x_{m+1} = i_{m+1} | x_m=i_m) = p_{i_m i_{m+1}}$

**Equi 2**: Given $x_m$ the future $\{x_n: n > m\}$ and the past $\{x_n: n < m\}$ are independent.

**Equi 3**: $\{x_n\}$ is a $\text{MC}(\lambda, P)$ if $P(x_0=i_0, \dots x_m=i_m) = \lambda p_{i_0i_1} p_{i_1i_2} \dots$


## Proof of equivalence

### Showing 3 $\implies$ 1

**Equi 3** $\implies$ **Equi 1.1** is obvious.

$$P(x_m=i_m | \text{PAST}) = P(x_m=i_m, \text{PAST})/P(\text{PAST})$$

From **Equi 3**, 

$$P(x_m=i_m | \text{PAST}) = \frac{\lambda_{i_0} \prod_{k=1}^{m}p_{i_{k-1},i_{k}}}{\lambda \prod_{k=1}^{m-1} p_{i_{k-1},i_{k}}} = p_{i_{m-1}, i_m}$$

which is **Equi 1.2**. 

Hence **Equi 3** $\implies$ **Equi 1**

### Showing 1 $\implies$ 3

$$P(x_m=i_m, \text{PAST}) = P(x_m=i_m|\text{PAST}) P(\text{PAST})$$

From **Equi 1.2**:

$$P(x_m=i_m | \text{PAST}) = p_{i_{m-1}, i_{m}}$$
$$\implies P(x_m=i_m, \text{PAST}) = p_{i_{m-1}, i_{m}}P(\text{PAST})$$

Now similarly pulling out each step from the past into the product, we get 

$$P(x_0=i_0, \dots x_m=i_m) = P(x_0=i_0) \prod_{\substack{k=m\\ \Delta k = -1}}^{1}p_{i_{k-1},i_{k}}$$

Finally, using **Equi 1.1**, we get **Equi 3**.

# Transition Matrix

$$P = ((p_{ij}))_{i,j \in S}$$

where $p_{ij} =$ probability that the chain jumps to state $j$ if it is in state $i$.

## Properties of the Transition Matrix

### Stochasticity

Row-wise sum is 1. $\sum_j p_{ij}$ is the sum of the probability that given we are at $i$, we jump to any possible $j$. Since we must be _somewhere_ every step, this sum must be 1.

### Stationarity

**Defn**: A stationary distribution on the nodes of the MC is such that $(x_0, ... x_n)$ has the same distribution as $(x_m, ..., x_{m+n})$ for all $m$. That is, $X_m \sim X_l$ for any $m$ and $l$.

$\mu_0(i) = P(x_0 = i) \forall i \in S$

$\mu_n(i) = P(x_n = i)$

$mu_1(i) = mu_0(i) * p_ji$

Which is $mu_i = mu_0 P^i$

A distribution $\pi$ on S is called Stationary / invariant distribution of the chain $\text{MC}(P)$ is $\pi = \pi P$
That is, $\pi$ is a left eigenvector of $P$ with eigenvalue 1.

> Exercise:
> Ehrenfest chain
> 
> Chain of length N.\
> $P(X_{n+1} = i+1 | X_n=i) = (N-i)/N; P(X_{n+1} = i-1 | X_n = i) = i/N$\
> Find $\pi$.


### Example

For $P = \begin{bmatrix}
1-\alpha & \alpha\\
\beta & 1-\beta
\end{bmatrix}$, what is the stationary distribution? What are the entries of $P^n$?

## Chapman Kolmogorov equation / Semigroup Property

$P^(n+m) = P^{n}P^{m} \forall n,m >= 0$

$$\begin{aligned}
p_{ij}^{(n+m)} &= P(X_{n+m} = j | X_0 = i)\\
            &= \sum_k P(X_{n+m}=j, X_m=k | X_0=i)\\
            &= \sum_k P(X_{n+m}=j | X_m=k , X_0=i) P(X_m=k | X_0=1)\\
            &= \sum_k P(X_{n+m}=j | X_m=k) P(X_m=k | X_0=1)\\
            &= \sum_k p^{m}_{ik} p_{kj}^{n}\\
\implies P^{n+m} &= P^nP^m
\end{aligned}$$

Going back to the example, 

$$P = (1-\alpha, \alpha; \beta, 1-\beta)$$

$$\begin{aligned}
    p_{11}^{(n)} &= \sum_j p_{1j}^{n-1} p_{j1}\\
                 &= p_{11}^{n-1}p_{11} + p_{12}^{n-1} p_{21}\\
                 &= p_{11}^{n-1} (1-\alpha) + \beta (1-p_{11}^{n-1})\\
\end{aligned}$$

> Exercise:
> 
> Similarly solve for other terms and find the values 
             
