---
title: Markov Chains
author: Dhruva Sambrani
date: 25 August, 2022
geometry: margin=1in
toc: true
---

\pagebreak

# Definition

**Equi 1**:
${X_n}_{n\ge 1}$ is a Markov Chain on a state space $S$ (countable) with an initial distribution $\lambda$ and transition matrix $P$ if

1. $P(x_0 = i) = \lambda_i$
2. **Markov property**: $P(x_{m+1} = i_{m+1} | \text{PAST}) = P(x_{m+1} = i_{m+1} | x_m=i_m) = p_{i_m i_{m+1}}$

**Equi 2**: Given $x_m$ the future $\{x_n: n > m\}$ and the past $\{x_n: n < m\}$ are independent.

**Equi 3**: $\{x_n\}$ is a $\text{MC}(\lambda, P)$ if $P(x_0=i_0, \dots x_m=i_m) = \lambda p_{i_0i_1} p_{i_1i_2} \dots$


## Proof of equivalence

### Showing 3 $\implies$ 1

**Equi 3** $\implies$ **Equi 1.1** is obvious.

$$P(x_m=i_m | \text{PAST}) = P(x_m=i_m, \text{PAST})/P(\text{PAST})$$

From **Equi 3**, 

$$P(x_m=i_m | \text{PAST}) = \frac{\lambda_{i_0} \prod_{k=1}^{m}p_{i_{k-1},i_{k}}}{\lambda \prod_{k=1}^{m-1} p_{i_{k-1},i_{k}}} = p_{i_{m-1}, i_m}$$

which is **Equi 1.2**. 

Hence **Equi 3** $\implies$ **Equi 1**

### Showing 1 $\implies$ 3

$$P(x_m=i_m, \text{PAST}) = P(x_m=i_m|\text{PAST}) P(\text{PAST})$$

From **Equi 1.2**:

$$P(x_m=i_m | \text{PAST}) = p_{i_{m-1}, i_{m}}$$
$$\implies P(x_m=i_m, \text{PAST}) = p_{i_{m-1}, i_{m}}P(\text{PAST})$$

Now similarly pulling out each step from the past into the product, we get 

$$P(x_0=i_0, \dots x_m=i_m) = P(x_0=i_0) \prod_{\substack{k=m\\ \Delta k = -1}}^{1}p_{i_{k-1},i_{k}}$$

Finally, using **Equi 1.1**, we get **Equi 3**.

# Transition Matrix

$$P = ((p_{ij}))_{i,j \in S}$$

where $p_{ij} =$ probability that the chain jumps to state $j$ if it is in state $i$.

## Stochasticity

Row-wise sum is 1. $\sum_j p_{ij}$ is the sum of the probability that given we are at $i$, we jump to any possible $j$. Since we must be _somewhere_ every step, this sum must be 1.


## Chapman Kolmogorov equation / Semigroup Property

$P^(n+m) = P^{n}P^{m} \forall n,m >= 0$

$$\begin{aligned}
p_{ij}^{(n+m)} &= P(X_{n+m} = j | X_0 = i)\\
            &= \sum_k P(X_{n+m}=j, X_m=k | X_0=i)\\
            &= \sum_k P(X_{n+m}=j | X_m=k , X_0=i) P(X_m=k | X_0=1)\\
            &= \sum_k P(X_{n+m}=j | X_m=k) P(X_m=k | X_0=1)\\
            &= \sum_k p^{m}_{ik} p_{kj}^{n}\\
\implies P^{n+m} &= P^nP^m
\end{aligned}$$

Going back to the example, 

$$P = (1-\alpha, \alpha; \beta, 1-\beta)$$

$$\begin{aligned}
    p_{11}^{(n)} &= \sum_j p_{1j}^{n-1} p_{j1}\\
                 &= p_{11}^{n-1}p_{11} + p_{12}^{n-1} p_{21}\\
                 &= p_{11}^{n-1} (1-\alpha) + \beta (1-p_{11}^{n-1})\\
\end{aligned}$$

> Exercise:
> 
> Similarly solve for other terms and find the values 

# Stationary distribution of an MC

**Defn**: A stationary distribution on the nodes of the MC is such that $(x_0, ... x_n)$ has the same distribution as $(x_m, ..., x_{m+n})$ for all $m$. That is, $X_m \sim X_l$ for any $m$ and $l$.

$\mu_0(i) = P(x_0 = i) \forall i \in S$

$\mu_n(i) = P(x_n = i)$

$mu_1(i) = mu_0(i) * p_ji$

Which is $mu_i = mu_0 P^i$

A distribution $\pi$ on S is called Stationary / invariant distribution of the chain $\text{MC}(P)$ is $\pi = \pi P$
That is, $\pi$ is a left eigenvector of $P$ with eigenvalue 1.

**Equi**:
S is finite, |S| = N, pi in |R^N_+ is called a stationary or invariant distn of the MS(P) if it satisfies
1. Balance Condition: piP = pi
2. pi mathbf 1 = 1

> Exercise:
> Ehrenfest chain
> 
> Chain of length N.\
> $P(X_{n+1} = i+1 | X_n=i) = (N-i)/N; P(X_{n+1} = i-1 | X_n = i) = i/N$\
> Find $\pi$.


### Example - 2 State MC

For $P = \begin{bmatrix}
1-\alpha & \alpha\\
\beta & 1-\beta
\end{bmatrix}$, what is the stationary distribution? What are the entries of $P^n$?

### Example - Bus Stop

Buses arrive at a bus stop st the inter-arrival times are iid. At time n, xn is the time until the next bus arrives

p_i+1, i = 1, p_1,i = q(i)

pi P = pi => left eigenvector with eigenvalue 1

[q(1)   q(2)    q(3) ...;
 1      0       0    ...;
 0      1       0    ...;
 ...
]

pi(i) = pi(i+1) + pi(1)q(i)


## Flow of a MC

For A \subset S, define F(A, A^C) = Sum_iinA sum_j in AC pi(i) pij

Thm: pi satisfies the Balance equation iff F(A, A^C) = F(A^C, A) forall A subset S

Pf: Suppose thm holds forall A subset S. For A={x},

F(A A^C) = sum_j!=k pi(k) p_kj 
F(A^C A) = sum_i!=k pi(i) p_ik
Then, 

sum_j!=k pi(k) p_kj = sum_i!=k pi(i) p_ik

sum_j!=k pi(k) p_kj = sum i in S pi(i) p_ik - pi(k)pkk

=> pi(k) sum_j in S pkj = sum_i in S pi(i)pik 
=> pi(k) = sum_{i in S} pi(i) p_ik


Con: pi(i) = sum j in S pi(j) p_ji
sum jinA pi(i)pij + sum_j in AC pi(i)pij = sumA pi(j)pji + sum_AC pi(j) p_ji
Sum over i in A on both sides and conclude

> Exercise: Consider the Gambler's Ruin MC with M=infty, reflecting boundary condition at 0.
> Take A = {0, 1, .. n-1} Write F(A,A^C) = F(A^C,A) and solve for pi

## Class structure of a MC

Def: i, j in S, i --> j if there exists n in N+{0} st p(ij)^n > 0  <==> there exists a PATH from i to j
 --> is transitive and reflexive

Def: <--> i<--> j "i communicates with j" iff i --> j and j --> i. <--> is an equivalence relation.

S = U_i C_i C_is are called communicating classes * reflexive because pii^0 > 0 forall i in S.

### Closed communicating Classes

Def: sum_j in C p_ij  = 1 for all i in C

If C is a closed communicating class then, 
i in C i --> j => j in C

i in C, i --> j => exists PATH 

