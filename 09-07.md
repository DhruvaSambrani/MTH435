$h_i^{j(n)} =  P_i(T_j=n) = f_{ij}^n$

To Prove:
$p_{ij}^n = \sum $f_{ij}^m p_{jj}^{(n-m)}

Proof:

$p_{ij}^n = P_i(X_n = j) = P(X_n=j | X_0=i)$

$= sum_{m=1}^n P_i (X_n=j, T_j=m)$

$= sum_{m=1}^n P_i (X_n=j| T_j=m)P(T_j = m)$

$= sum_{m=1}^n f_{ij}^m P(T_j = m)$

$= sum_{m=1}^n f_{ij}^m p_{jj}^{(n-m)}$ } from SMP

Define $N_i = \sum_{k=1}^\infty \mathbf{I}_{X_n = i}$ = no of visits to state $i$

**Proposition**:
$$P_i(N_j = k) = \begin{cases}
    1 - f_{ij} & k=0
    f_{ij} (1-f_{jj}) f_{jj}^k-1 & k>0
\end{cases}$$

Where $f_{ij} = P_i(T_j < \infty)$.

For $k = 0$,

$P_i(N_j = 0) = P_i(T_j = \infty)$

$= 1 - P_i(T_j < \infty)$

$= 1 - f_{ij}$

For $k>0$

**Proof by induction**:

$P_i(N_j = k)    = P_i(T_j^{k+1} - T_j^k = \infty, T_j < \infty)$

$= P_i(T_j^{k+1} - T_j = \infty | T_j^k < \infty) P_i(T_j^\infty < \infty)$

$= P_j(T_j = \infty) P_i(T_j^k < \infty)$

$= (1 - f_{jj}) P_i(T_j^k < \infty)$

$= (1-f_{jj}) P_i(N_j >= k)$

$= (1 -f_{jj}) (1 - \sum_{r=0}^{k-1} P_i(N_j = r))$

$= (1 -f_{jj}) (1 - [1 - f_{ij} + \sum_{r=0}^{k-1} f_{ij} (1-f_{jj}) f_{jj}^{r-1} ])$ from induction hypothesis

$= (1 - f_{jj}) (f_{ij} - f_{ij} (f_{jj}^0 - f_{jj}^{k-1}))$

$= (1 - f_{jj}) (f_{ij} f_{jj}^{k-1})$

$P_i(N_i >= k) ?=$

# Recurrence and Transience of a Markov Chain

$i \in S$ is called recurrent if $P(X_n = i \text{ for infinitely many} n) = 1$

it is called transient if $P(X_n = i \text{for infinitely many} n) = 0$

**Theorem**: TFAE

1. $i in S$ is recurrent
2. $f_ii$ = 1
3. $P_i(N_i = \infty) = 1$
4. $E_i[N_i] =\infty$
5. $\sum_n p_{ii}^(n) = \infty$

1. 1 $\iff$ 3 by defn
2. $3 \iff 4$
    a. $3\implies 4$ is obvious
    b. $4 \implies 3$ requires the fact that $N_i ~ \text{Geom}(f_{ii}) \implies P_i(N_i = \infty) = 1$
3. $1 \iff 2$ - $f_{ii} = P_i(T_i < \infty) = 1$; so if $P(X_n = i) = 1$ for some $n$, it must hit $i$ again in finite time.
4. $4 \iff 5$ - $E_i[\sum_{n=1}^\infty I_{X_n = i}] = \sum_n P_i(X_n = i) = \sum_n p_{ii}^n$

**Theorem**: TFAE

1. $i in S$ is transient
2. $f_ii$ < 1
3. $P_i(N_i < \infty) = 1$
4. $E_i[N_i] <\infty$
5. $\sum_n p_{ii}^(n) < \infty$

In particular, $i$ transient $\implies \lim_{n\to\infty} p_{ii}^(n) = 0$

Proposition: If $j$ is transient then $p_{ij}^(n) \to 0$ as $n \to \infty$.

Recall 

$p_{ij}^n   = sum_{m=1}^n f_{ij}^m p_{jj}^{(n-m)}$

$\sum_n p_{ij}^n = \sum_n \sum_{m=1}^n f_{ij}^m p_{jj}^{(n-m)}$

$= \sum_m f_ij^{(m)} \sum_{n=m}^\infty p_{jj}^{(n-m)}$

$= (1 - E_j[N_j]) sum_{m=1}^\infty f_{ij}^{(m)}$

$= (1 - E_j[N_j]) P_i(T_j < \infty)$

$< \infty$

$\implies p_{ij}^n \to 0$ as $n -> \infty$

## Transience is a class property

Let $i \in S$ be transient. $C$ be communicating class of $i$ and take $j \in C$.

$\exists n, m st p_{ij}^{(n)}, p_{ji}^{(m)} > 0$

$\forall r >= 0; p_{ii}{(n+r+m)} \ge p_{ij}^n p_{jj}^r p_{ji}^m$

$sum_r^\infty p_{jj}^(r) \le \frac{1}{p_{ii}^n p_{ji}^m} \sum_{r=0}^\infty p_{ii}^{(n+r+m)} < \infty$ because i is transient.

$\implies p_{jj}^{(r)} \to 0$ as $r \to \infty$.

Hence, $j$ is also transient.

## Recurrent classes are closed

Let $i \in C$; $C$ not closed.

Then $\exists j st i \rightarrow j; j \nrightarrow i$;

Then $\exists m st p_{ij}^{(m)} > 0$;

$P_i(X_n = i, X_m = j) = P_i(X_n = i | X_m = j) P_i(X_m = j) = 0$

P_i(X_n = i for infinitely many n)  = P_i(A_n) 
                                    = P_i({X_m = j} CAP A_n) + P_i({X_m != j} CAP A_n)
                                    = P_i({X_m != j} CAP A_n)
                                    <= P_i(X_m != j) = 1 - P_i(X_m = j) < 1

Therefore i is not recurrent.

## Positive Recurrence and Null Recurrence

$i \in S$, $i$ recurrent, is positive recurrent if $E_i[T_i] < \infty$
$i \in S$, $i$ recurrent, is null recurrent if $E_i[T_i] = \infty$

### Connection with the stationary distribution

Aim: 

1. Irreducability + Recurrence => balance equation is satisfied
2. Irreducability + Positive Recurrence => normalization is satisfied

### Harmonic Functions

$h : S -> RR$ is a harmonic wrt to P where P is row stochastic if $h(x) = sum_{y\in S} p_{xy} h(y)$

If P is irreducable, h is a constant function. 

Since S is finite, h attains maximum at some point say $x_0$

$h(x_0) >= h(x) \forall x \in S$

Let z in S, st p_(x_0z) > 0 and suppose h(z) < h(x_0).

h(x_0) = sum_x in S p_x0x h(x) = p_x0z h(z) + sum_x!=z p_{x_0 x} h(x) < h(x_0) sum_{x in S} p_x0x = h(x_0)

That means, h(z) >= h(x_0) => h(z) = h(x_0)

Now show that any z' in S, Then exists PATH x_0 -> z', and h(k) = h(x_0) for each successive $k$.

If P h = h
(P - I) h = 0

h = cI 
=> dim(Ker(P-I)) = 1
=> dim(Ker(P^T-I)) = 1
=> (P^T - I) v^T = 0 has 1 dimensional solution
=> vP = v has 1 dimensional solution.

Hence any pi P = pi has to a constant multiple of v.

But due to normalization there is only one invariant probability distribution, 

pi = v

## Existance and Uniqueness of invariant measure

Def: Y_i^a = E[sum_n=0^{T_a - 1} I_{X_n = i}]

Thm:
Let MC(P) be irreducable and recurrent, the following hold

1. Y_a^a = 1
2. Y^a P = Y^a, where Y^a = (Y^a_i)
3. $0 < Y_i^a < \infty \forall i \in S$

1 is obvious.

Y_i^a   = E_a[sum_n=0^T_a-1 I_{X_n = i}]
        = E_a[sum_m=1^infty sum_n=0^{m-1} I_{X_n = i, T_a = m}]
        = E_a[sum_n=1^{infty} I_{X_n = i, T_a >= n}]       ------------------------ (1)
        = sum_n=1^infty sum_j P_a(X_n=i, X_{n-1} =j, T_a >= n)
        = sum_n=1^infty sum_j P_a(X_n=i | X_{n-1} =j, T_a >= n) P_a(X_m=j, T_a >= n)
        = sum_n=1^infty sum_j p_{ji} P_a(X_{n-1}=j, T_a >= n)

sum_n=1^infty P(X_{n-1}=j, T_a >= n) 
    = E_a[sum_{n=1}^infty I_{X_n-1 = j, T_a>=n}]
    = E_a[sum_{n=0}^infty I_{X_n = j, T_a>=(n+1)}]
    = Y_j^a

Hence, 2 holds

3. For i in S exists n1, n2 > 0 st

p_ia^(n1), p_ai^(n2) > 0

Y^aP^n = Y^a
=> Y_i^a >= p_ai^(n_2) Y_a^a > 0 

Thm: If lambda is invariant measure of irr MC, lambda_a = 1 for some a. Then lambda >= Y^a. If chain is also recurrent,
lambda = Y^a

lambda P = lambda

lambda_j = sum p_i0j lambda_i0 = sum_i0!=a p_i0j + p_aj lambda_a
    = sum_i0!=a p_i0j sum_i1 pi1i0 lambda_i1  + p_aj lambda_a
    = sum_i0!=a p_i0j sum_i1!=a pi1i0 lambda_i1  + sum_i0!=a p_i0j p_ai0 lambda_a + p_aj lambda_a
    = [sum_i0!=a ... sum_i_n-1!=a   ] ------------------------------- (2)
        + p_aj lambda_a 
        + sum_i0!=a p_ai0 pi_0j lambda_a
        + sum_i1!=a sum_i0!=a p_ai1 pi1i0 pi_0j lambda_a
        + ...

(2) >= 0, and hence, 

lambda_j    >= lambda_a [p_aj + sum_i0!=a p_ai0 p_i0j  ...]
            = P_a(X1=j, T_a>=1) + P_a(X2=j, T_a>=2) + ...
            = Y_j^a ------------------------------- from (1)

If chain is recurrent, 
Define mu_j = lambda_j - lambda_a Y_j^a 

0 = mu_a = sum_i in S mu_i p_ia^(n)
n st p_ja^(n) > 0 

sum_i in S mu_i p_ia^(n) >= mu_i p_ja^(n)

=> mu_j = 0 

This can be done for any k in S since the chain is irreducable. => mu = 0

## Invariant Probability Distribution

Thm: Consider an irreducable MC(S, P)

1. Some state i in S is positive recurrent
2. All states are positive recurrent
3. The chain has an invariant probability distribution lambda.

If the above holds, lambda= 1/E_i[T_i] forall i in S

Proof:
2 => 1 is obvious

1 => 3 
i in S is recurrent, then MC is recurrent.
=> Y^i is an invariant measure.

Sum_k in S Y^i_k = sum_k E_i [sum_n=0^T_a-1 I_{X_n =k}] = E_i [sum_n^{T_a-1} sum_k I_{X_n=k}]
                = E_i[T_i]

If positive recurrent, E_i[T_i] < infinity.

Hence \lambda_j = Y^i_j / E_i[T_i] is an invariant probability distribution.

3 => 2

Since lambda is pdist, exists j st lambda_j > 0.

=> lambda_k = sum p_ik^(n) lambda_i forall n, j.

Pick an n such that p_jk^(n) > 0.

This implies lambda_k > 0 for all k.

Fix a in S, 

pi_i = lambda_i/lambda_a

From before, 

Y_i^a <= pi_i = lambda_i/lambda_a. 

Summing over all i in S we get,

E_a[T_a] <= 1/lambda_a < infty

=> a is positive recurrent. 

But we can fix any a, since lambda_i > 0 for all i.

Hence, all a in S are positive recurrent.

# Law of large numbers

Consider irr MC(alpha, P).
Define N_i(n) = sum_k=0^n-1 I_{X_k=i}

1. If the chain is transient or Null recurrent, 

N_i(n)/n ---al.sure. as n to infty---> 0

2. If the chain is positive recurrent with invariant pdist lambda.

N_i(n)/n  ---al.sure. as n to infty---> 1/E_i[T_i]

T_i^(r) = inf{n>T_i^{r-1}; X_n=i}

S_1 = T_i^(1)
S_2 = T_i^(2) - T_i^(1)

{S_j} are iid.



